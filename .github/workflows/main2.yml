name: Crawl and Convert Full Site Content to Text PDF

on:
  workflow_dispatch:
  schedule:
    - cron: "0 2 * * *" # Runs daily at 2 AM UTC

jobs:
  crawl-and-extract:
    runs-on: ubuntu-latest
    timeout-minutes: 90

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Install dependencies
        run: |
          npm install axios cheerio p-limit pdfkit

      - name: Crawl website and extract URLs (2 levels deep)
        run: |
          node <<'EOF'
          const { URL } = require('url');
          const axios = require('axios');
          const cheerio = require('cheerio');
          const fs = require('fs');
          const startUrl = 'https://www.lhh.com/en-us';
          const domain = new URL(startUrl).hostname;
          const visited = new Set();
          const urls = new Set([startUrl]);
          const maxDepth = 2;

          async function crawl(url, depth) {
            if (depth > maxDepth || visited.has(url)) return;
            visited.add(url);
            try {
              console.log(`üåê Crawling [${depth}]: ${url}`);
              const res = await axios.get(url, { timeout: 20000 });
              const $ = cheerio.load(res.data);
              $('a[href]').each((_, el) => {
                let href = $(el).attr('href');
                if (!href) return;
                if (href.startsWith('/')) href = `https://${domain}${href}`;
                if (!href.startsWith('https://www.lhh.com/en-us')) return;
                if (!visited.has(href)) urls.add(href);
              });
            } catch (err) {
              console.error(`‚ö†Ô∏è Failed to crawl ${url}: ${err.message}`);
            }
            if (depth < maxDepth) {
              for (const next of [...urls]) {
                if (!visited.has(next)) await crawl(next, depth + 1);
              }
            }
          }

          (async () => {
            await crawl(startUrl, 1);
            fs.writeFileSync('urls.txt', Array.from(urls).join('\n'));
            console.log(`‚úÖ Found ${urls.size} URLs`);
          })();
          EOF

      - name: Extract text content and generate PDFs
        run: |
          mkdir -p pdfs
          node <<'EOF'
          const fs = require('fs');
          const axios = require('axios');
          const cheerio = require('cheerio');
          const PDFDocument = require('pdfkit');
          const pLimit = require('p-limit').default;

          const urls = fs.readFileSync('urls.txt', 'utf8').trim().split('\n').filter(Boolean);
          const limit = pLimit(3);

          (async () => {
            for (const url of urls) {
              await limit(async () => {
                try {
                  console.log(`üìù Extracting content from: ${url}`);
                  const res = await axios.get(url, { timeout: 20000 });
                  const $ = cheerio.load(res.data);

                  // Extract text content
                  let title = $('h1').first().text().trim() || $('title').text().trim();
                  let content = '';

                  $('h1, h2, h3, h4, h5, h6, p, li').each((_, el) => {
                    const tag = el.tagName.toLowerCase();
                    const text = $(el).text().trim();
                    if (!text) return;
                    if (['h1','h2','h3'].includes(tag)) content += `\n\n# ${text}\n`;
                    else if (tag === 'li') content += `‚Ä¢ ${text}\n`;
                    else content += `${text}\n\n`;
                  });

                  if (!content) {
                    console.log(`‚ö†Ô∏è No readable content found at: ${url}`);
                    return;
                  }

                  // Create clean PDF
                  const safe = url.replace(/^https?:\/\//, '').replace(/[\/:?&=#]/g, '_');
                  const pdfPath = `pdfs/${safe}.pdf`;
                  const doc = new PDFDocument({ margin: 50 });
                  const stream = fs.createWriteStream(pdfPath);
                  doc.pipe(stream);

                  doc.fontSize(18).text(title, { underline: true });
                  doc.moveDown();
                  doc.fontSize(12).text(content, { align: 'left' });

                  doc.end();
                  await new Promise(res => stream.on('finish', res));

                  console.log(`‚úÖ Saved clean text PDF: ${pdfPath}`);
                } catch (err) {
                  console.error(`‚ùå Failed to extract ${url}: ${err.message}`);
                }
              });
            }
          })();
          EOF

      - name: Upload Clean Text PDFs
        uses: actions/upload-artifact@v4
        with:
          name: lhh-text-pdfs
          path: pdfs/
