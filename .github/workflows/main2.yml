name: Crawl and Convert Full Site to PDF (Puppeteer)

on:
  workflow_dispatch:
  schedule:
    - cron: "0 2 * * *" # Runs daily at 2 AM UTC

jobs:
  crawl-and-pdf:
    runs-on: ubuntu-latest
    timeout-minutes: 90

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Install dependencies
        run: |
          npm install puppeteer p-limit cheerio axios

      - name: Crawl website and extract URLs (2 levels deep)
        run: |
          node <<'EOF'
          const { URL } = require('url'); // ‚úÖ Fix for Node.js v20
          const axios = require('axios');
          const cheerio = require('cheerio');
          const fs = require('fs');

          const startUrl = 'https://www.lhh.com/en-us';
          const domain = new URL(startUrl).hostname;
          const visited = new Set();
          const urls = new Set([startUrl]);
          const maxDepth = 2;

          async function crawl(url, depth) {
            if (depth > maxDepth || visited.has(url)) return;
            visited.add(url);

            try {
              console.log(`üåê Crawling [${depth}]: ${url}`);
              const res = await axios.get(url, { timeout: 20000 });
              const $ = cheerio.load(res.data);

              $('a[href]').each((_, el) => {
                let href = $(el).attr('href');
                if (!href) return;
                if (href.startsWith('/')) href = `https://${domain}${href}`;
                if (!href.startsWith('https://www.lhh.com/en-us')) return;
                if (!visited.has(href)) urls.add(href);
              });
            } catch (err) {
              console.error(`‚ö†Ô∏è Failed to crawl ${url}: ${err.message}`);
            }

            if (depth < maxDepth) {
              for (const next of [...urls]) {
                if (!visited.has(next)) await crawl(next, depth + 1);
              }
            }
          }

          (async () => {
            await crawl(startUrl, 1);
            const urlList = Array.from(urls);
            fs.writeFileSync('urls.txt', urlList.join('\n'));
            console.log(`‚úÖ Found ${urlList.length} URLs`);
          })();
          EOF

      - name: Generate PDFs using Puppeteer
        run: |
          mkdir -p pdfs
          node <<'EOF'
          const fs = require('fs');
          const puppeteer = require('puppeteer');
          const pLimit = require('p-limit').default;

          const urls = fs.readFileSync('urls.txt', 'utf8').trim().split('\n').filter(Boolean);
          const limit = pLimit(3); // 3 pages in parallel

          (async () => {
            const browser = await puppeteer.launch({
              headless: true,
              args: ['--no-sandbox', '--disable-setuid-sandbox']
            });

            const tasks = urls.map(url => limit(async () => {
              const page = await browser.newPage();
              await page.setViewport({ width: 1280, height: 800 });
              try {
                console.log(`üìÑ Rendering: ${url}`);
                await page.goto(url, { waitUntil: 'networkidle2', timeout: 60000 });
                await new Promise(res => setTimeout(res, 3000)); // wait for JS content

                const safe = url.replace(/^https?:\/\//, '').replace(/[\/:?&=#]/g, '_');
                const pdfPath = `pdfs/${safe}.pdf`;
                await page.pdf({
                  path: pdfPath,
                  format: 'A4',
                  printBackground: true,
                  margin: { top: '20mm', bottom: '20mm', left: '15mm', right: '15mm' }
                });

                console.log(`‚úÖ Saved: ${pdfPath}`);
              } catch (err) {
                console.error(`‚ùå Failed: ${url} ‚Äî ${err.message}`);
              } finally {
                await page.close();
              }
            }));

            await Promise.all(tasks);
            await browser.close();
            console.log('üèÅ All PDFs generated successfully!');
          })();
          EOF

      - name: Upload PDFs as artifacts
        uses: actions/upload-artifact@v4
        with:
          name: lhh-fullsite-pdfs
          path: pdfs/
