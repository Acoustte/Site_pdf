name: Crawl and Convert Full Site to Clean Text PDF

on:
  workflow_dispatch:
  schedule:
    - cron: "0 2 * * *" # Daily at 2 AM UTC

jobs:
  crawl-and-convert:
    runs-on: ubuntu-latest
    timeout-minutes: 90

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Install dependencies
        run: |
          npm install axios cheerio jsdom @mozilla/readability pdfkit p-limit

      - name: Crawl website (2 levels deep)
        run: |
          node <<'EOF'
          const { URL } = require('url');
          const axios = require('axios');
          const cheerio = require('cheerio');
          const fs = require('fs');

          const startUrl = 'https://www.lhh.com/en-us';
          const domain = new URL(startUrl).hostname;
          const visited = new Set();
          const urls = new Set([startUrl]);
          const maxDepth = 2;

          async function crawl(url, depth) {
            if (depth > maxDepth || visited.has(url)) return;
            visited.add(url);
            try {
              console.log(`üåê Crawling (${depth}): ${url}`);
              const res = await axios.get(url, { timeout: 20000 });
              const $ = cheerio.load(res.data);
              $('a[href]').each((_, el) => {
                let href = $(el).attr('href');
                if (!href) return;
                if (href.startsWith('/')) href = `https://${domain}${href}`;
                if (href.startsWith('https://www.lhh.com/en-us') && !visited.has(href)) {
                  urls.add(href);
                }
              });
            } catch (err) {
              console.error(`‚ö†Ô∏è Failed to crawl ${url}: ${err.message}`);
            }
            if (depth < maxDepth) {
              for (const next of [...urls]) {
                if (!visited.has(next)) await crawl(next, depth + 1);
              }
            }
          }

          (async () => {
            await crawl(startUrl, 1);
            fs.writeFileSync('urls.txt', Array.from(urls).join('\n'));
            console.log(`‚úÖ Found ${urls.size} URLs`);
          })();
          EOF

      - name: Extract readable text & generate PDFs
        run: |
          mkdir -p pdfs
          node <<'EOF'
          const fs = require('fs');
          const axios = require('axios');
          const { JSDOM } = require('jsdom');
          const { Readability } = require('@mozilla/readability');
          const PDFDocument = require('pdfkit');
          const pLimit = require('p-limit').default;

          const urls = fs.readFileSync('urls.txt', 'utf8').trim().split('\n').filter(Boolean);
          const limit = pLimit(3);

          (async () => {
            for (const url of urls) {
              await limit(async () => {
                try {
                  console.log(`üß† Extracting: ${url}`);
                  const res = await axios.get(url, { timeout: 30000 });
                  const dom = new JSDOM(res.data, { url });
                  const reader = new Readability(dom.window.document);
                  const article = reader.parse();

                  if (!article || !article.textContent.trim()) {
                    console.log(`‚ö†Ô∏è No readable content found at: ${url}`);
                    return;
                  }

                  const title = article.title || 'Untitled Page';
                  const safeName = url.replace(/^https?:\/\//, '').replace(/[\/:?&=#]/g, '_');
                  const pdfPath = `pdfs/${safeName}.pdf`;

                  // Create formatted text PDF
                  const doc = new PDFDocument({ margin: 50 });
                  const stream = fs.createWriteStream(pdfPath);
                  doc.pipe(stream);

                  doc.fontSize(18).text(title, { underline: true });
                  doc.moveDown();
                  doc.fontSize(12).text(article.textContent.trim(), {
                    align: 'left',
                    lineGap: 4,
                  });

                  doc.end();
                  await new Promise(res => stream.on('finish', res));
                  console.log(`‚úÖ Saved clean PDF: ${pdfPath}`);
                } catch (err) {
                  console.error(`‚ùå Failed to process ${url}: ${err.message}`);
                }
              });
            }
          })();
          EOF

      - name: Upload PDFs as artifacts
        uses: actions/upload-artifact@v4
        with:
          name: lhh-clean-text-pdfs
          path: pdfs/
